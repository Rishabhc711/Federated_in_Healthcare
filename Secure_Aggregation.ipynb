{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Secure Aggregation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCfYwtSXiqWhWUCS8yx3uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabhc711/Federated_in_Healthcare/blob/main/Secure_Aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ3QZ4BvEPQz",
        "outputId": "50cebf65-59f9-44c8-b628-57e3ee748a4c"
      },
      "source": [
        "!pip install -q syft==0.1.29a1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 327kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 522kB 44.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 43.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7MB 49.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 676.9MB 25kB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 110.5MB 56kB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 41.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 42.9MB/s \n",
            "\u001b[?25h  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.5 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQGtYuYaEjIo",
        "outputId": "cf04ee89-e8ae-433e-df1d-638518f4d00a"
      },
      "source": [
        "!pip install -q numpy==1.16.4 matplotlib==3.0.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 17.3MB 265kB/s \n",
            "\u001b[K     |████████████████████████████████| 13.0MB 189kB/s \n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuwORPepEjK3",
        "outputId": "57816ed3-6eb8-4e7c-e170-16cb4983dda7"
      },
      "source": [
        "!pip install -q torch==1.1.0 torchvision==0.3.0 torchsummary==1.5.1 torchtext==0.3.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20kB 21.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 30kB 12.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 61kB 10.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3dB8CRZfEjNS",
        "outputId": "a958aecc-5b12-43a0-f6f0-06a8526d2981"
      },
      "source": [
        "import torch as th\n",
        "th.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezgRh5PnEjPl",
        "outputId": "5966425e-50bc-48eb-d82c-6a780daf8bf3"
      },
      "source": [
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import syft as sy\n",
        "import helper\n",
        "\n",
        "#Hooking syft to torch\n",
        "hook = sy.TorchHook(th)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.7/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.5.so'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ery_IzBZEjSJ"
      },
      "source": [
        "#4 different models to pose as 4 different clents\n",
        "def create_workers():\n",
        "  workers = []\n",
        "  cartman = sy.VirtualWorker(hook, id = \"rishabh\")\n",
        "  workers.append(rishabh)\n",
        "  kyle = sy.VirtualWorker(hook, id = \"pranav\")\n",
        "  workers.append(pranav)\n",
        "  kenny = sy.VirtualWorker(hook, id = \"divyanshu\")\n",
        "  workers.append(divyanshu)\n",
        "  stan = sy.VirtualWorker(hook, id = \"saumya\")\n",
        "  workers.append(saumya)\n",
        "  return workers\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Z3wTwbEjUi"
      },
      "source": [
        "#to clear out every tensor stored in the list of virtual workers\n",
        "def clear_workers(workers):\n",
        "  for worker in workers:\n",
        "    worker.clear_objects()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAhEGSYsEjW8"
      },
      "source": [
        "#Split datasets to the clients\n",
        "def create_federated_and_test_loaders(workers, trainset, testset):\n",
        "  federated_train_loader = sy.FederatedDataLoader(\n",
        "      trainset.federate(workers), \n",
        "      batch_size=32, shuffle=True)\n",
        "\n",
        "  test_loader = th.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return federated_train_loader, test_loader"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMahXc9oEjZM"
      },
      "source": [
        "#Method to bind remote optimizer functions to the Initial states of the remote models \n",
        "def create_models(workers, lr):\n",
        "  remoteModels = list()\n",
        "  remoteOptimizers = list()\n",
        "  for worker in workers:\n",
        "    model = classifier()\n",
        "    model = model.send(worker)\n",
        "    remoteOptimizers.append(optim.SGD(model.parameters(), lr))\n",
        "    remoteModels.append(model)\n",
        "  return remoteModels, remoteOptimizers\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2hrqkJNEjbd"
      },
      "source": [
        "#Method to train models on the virtual workers without moving any gradients to the central model until the gradients have been collated.\n",
        "def create_train_federated_models(workers, loader, lr = 0.12, epoch = 5):\n",
        "  #sends model to first virtual worker\n",
        "  virtual_models, virtual_optimizers = create_models(workers, lr)\n",
        "#   virtual_model = classifier().send(workers[0])\n",
        "#   optimizer = optim.SGD(virtual_model.parameters(), lr)\n",
        "  criterion = nn.NLLLoss()\n",
        "  for n in range(epoch):\n",
        "    \n",
        "    #Integer to keep up with first index.\n",
        "    i = 0\n",
        "    \n",
        "    #Integer to keep up with current worker while training\n",
        "    j = 0\n",
        "    \n",
        "    #Integer to count number of mini-batches per worker\n",
        "    n_mbatch = 0\n",
        "    \n",
        "    #Variable to keep up with current worker while looping\n",
        "    dbLoc = None\n",
        "    \n",
        "    #Variable to store cummulative loss.\n",
        "    cum_loss = 0\n",
        "    \n",
        "    \n",
        "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
        "      \n",
        "      #condition to set dbLoc to the first worker\n",
        "      if i == 0:\n",
        "        i = 2\n",
        "        dbLoc = imgs.location\n",
        "        \n",
        "      #condition to change dbLoc if img is stored on a different worker and also calculate loss\n",
        "      if dbLoc is not imgs.location:\n",
        "        print(\"The total loss for {0} for epoch {2} is {1}\".format(workers[j].id, cum_loss / n_mbatch, n+1))\n",
        "        dbLoc = imgs.location\n",
        "        j += 1\n",
        "        \n",
        "        #Moving the model to a new worker\n",
        "#         virtual_model.move(dbLoc)\n",
        "        \n",
        "        #Resetting the cummulative loss and batch count to zero for new worker\n",
        "        cum_loss = 0\n",
        "        n_mbatch = 0\n",
        "\n",
        "      virtual_optimizers[j].zero_grad()\n",
        "      output = virtual_models[j].forward(imgs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      virtual_optimizers[j].step()\n",
        "      cum_loss +=  loss.get().item()\n",
        "      n_mbatch += 1\n",
        "    print(\"The total loss for {0} is {1}\".format(workers[j].id, cum_loss / n_mbatch))\n",
        "  return virtual_models\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LiSr2xqEje_"
      },
      "source": [
        "#to return the model to the central server\n",
        "def create_central_model(model):\n",
        "  return model.get()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtmBY1TgFONe"
      },
      "source": [
        "#Get gradients of client models to their integer representation, encrypt and distribute among the provided workers\n",
        "def share_gradients(models, workers):\n",
        "  for model in models:\n",
        "    model.fc1.weight.data = model.fc1.weight.data.fix_prec().share(*workers)\n",
        "    model.fc1.bias.data = model.fc1.bias.data.fix_prec().share(*workers)\n",
        "    model.fc2.weight.data = model.fc2.weight.data.fix_prec().share(*workers)\n",
        "    model.fc2.bias.data = model.fc2.bias.data.fix_prec().share(*workers)\n",
        "    model.fc3.weight.data = model.fc3.weight.data.fix_prec().share(*workers)\n",
        "    model.fc3.bias.data = model.fc3.bias.data.fix_prec().share(*workers)\n",
        "    model.fc4.weight.data = model.fc4.weight.data.fix_prec().share(*workers)\n",
        "    model.fc4.bias.data = model.fc4.bias.data.fix_prec().share(*workers)\n",
        "    model.fc5.weight.data = model.fc5.weight.data.fix_prec().share(*workers)\n",
        "    model.fc5.bias.data = model.fc5.bias.data.fix_prec().share(*workers)   \n",
        "  return models\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uX0vRmdFOPz"
      },
      "source": [
        "#Method to aggregate the gradients accross the shared devices and return the parameters\n",
        "def aggregate_grads(models):\n",
        "  fc1_weight = list()\n",
        "  fc1_bias = list()\n",
        "  fc2_weight = list()\n",
        "  fc2_bias = list()\n",
        "  fc3_weight = list()\n",
        "  fc3_bias = list()\n",
        "  fc4_weight = list()\n",
        "  fc4_bias = list()\n",
        "  fc5_weight = list()\n",
        "  fc5_bias = list()\n",
        "  for model in models:\n",
        "    fc1_weight.append(model.fc1.weight.data.clone().get())\n",
        "    fc1_bias.append(model.fc1.bias.data.clone().get())\n",
        "    fc2_weight.append(model.fc2.weight.data.clone().get())\n",
        "    fc2_bias.append(model.fc2.bias.data.clone().get())\n",
        "    fc3_weight.append(model.fc3.weight.data.clone().get())\n",
        "    fc3_bias.append(model.fc3.bias.data.clone().get())\n",
        "    fc4_weight.append(model.fc4.weight.data.clone().get())\n",
        "    fc4_bias.append(model.fc4.bias.data.clone().get())\n",
        "    fc5_weight.append(model.fc5.weight.data.clone().get())\n",
        "    fc5_bias.append(model.fc5.bias.data.clone().get())\n",
        "  params = {}\n",
        "  params[\"fc1.weight\"] = (sum(fc1_weight) / len(fc1_weight)).get().float_prec()\n",
        "  params[\"fc1.bias\"] = (sum(fc1_bias) / len(fc1_bias)).get().float_prec()\n",
        "  params[\"fc2.weight\"] = (sum(fc2_weight) / len(fc2_weight)).get().float_prec()\n",
        "  params[\"fc2.bias\"] = (sum(fc2_bias) / len(fc2_bias)).get().float_prec()\n",
        "  params[\"fc3.weight\"] = (sum(fc3_weight) / len(fc3_weight)).get().float_prec()\n",
        "  params[\"fc3.bias\"] = (sum(fc3_bias) / len(fc3_bias)).get().float_prec()\n",
        "  params[\"fc4.weight\"] = (sum(fc4_weight) / len(fc4_weight)).get().float_prec()\n",
        "  params[\"fc4.bias\"] = (sum(fc4_bias) / len(fc4_bias)).get().float_prec()\n",
        "  params[\"fc5.weight\"] = (sum(fc5_weight) / len(fc5_weight)).get().float_prec()\n",
        "  params[\"fc5.bias\"] = (sum(fc5_bias) / len(fc5_bias)).get().float_prec()\n",
        "  return params\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6bVKsAiFOSC"
      },
      "source": [
        "#Method to update the central model with the aggregated parameters from the remote models\n",
        "def update_central_model(params):\n",
        "  model = classifier()\n",
        "  model.fc1.weight.data = params[\"fc1.weight\"]\n",
        "  model.fc1.bias.data = params[\"fc1.bias\"]\n",
        "  model.fc2.weight.data = params[\"fc2.weight\"]\n",
        "  model.fc2.bias.data = params[\"fc2.bias\"]\n",
        "  model.fc3.weight.data = params[\"fc3.weight\"]\n",
        "  model.fc3.bias.data = params[\"fc3.bias\"]\n",
        "  model.fc4.weight.data = params[\"fc4.weight\"]\n",
        "  model.fc4.bias.data = params[\"fc4.bias\"]\n",
        "  model.fc5.weight.data = params[\"fc5.weight\"]\n",
        "  model.fc5.bias.data = params[\"fc5.bias\"]  \n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzXE3nzeFOUq"
      },
      "source": [
        "#Classifier for creating the models\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
        "    return x\n",
        " "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "8Mn-wI4lFOW8",
        "outputId": "6e05afa3-ee4f-443b-a02b-fcf1f11f8652"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6dc691aac064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmnist_trainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmnist_testset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_gzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mextract_gzip\u001b[0;34m(gzip_path, remove_finished)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mout_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/MNIST/raw/train-images-idx3-ubyte.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkJm12IdFOZZ"
      },
      "source": [
        "train = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ]))\n",
        "  \n",
        "\n",
        "test = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.5,), (0.5,))\n",
        "                             ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZmgbHNRFOc3"
      },
      "source": [
        "from syft import VirtualWorker\n",
        "workers = create_workers()\n",
        "clear_workers(workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CD_GbNfFq2q"
      },
      "source": [
        "federatedset, testset= create_federated_and_test_loaders(workers, mnist_trainset, mnist_testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOxxhqs3Fq5Q"
      },
      "source": [
        "remoteModels = create_train_federated_models(workers, federatedset, lr = 0.15, epoch = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUgdOoLxFq7z"
      },
      "source": [
        "remoteModels = create_train_federated_models(workers, federatedset, lr = 0.15, epoch = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoAFHPXOFq_H"
      },
      "source": [
        "params  = aggregate_grads(remoteModels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo5pgilCF0jP"
      },
      "source": [
        "central_model = update_central_model(params)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}